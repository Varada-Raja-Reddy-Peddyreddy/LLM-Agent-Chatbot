{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import Api Keys from Secret Api Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret_api_keys import Groq_API_Key\n",
    "from secret_api_keys import riza_api_key\n",
    "from secret_api_keys import Tavily_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set environment varialbes for api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RIZA_API_KEY\"] = riza_api_key\n",
    "os.environ[\"TAVILY_API_KEY\"] = Tavily_api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import necessary modules for typed dictionaries , state graph and message handiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated # import the Annoted class for type hints with additional metadata\n",
    "from typing_extensions import TypedDict # import the TypeDict class for defining custom typed Dictionaries\n",
    "\n",
    "from langgraph.graph import StateGraph # import the StateGraph  class for create State Graph\n",
    "from langgraph.graph.message import add_messages # import the Add_message function for adding messages to list \n",
    "from langgraph.prebuilt import ToolNode , tools_condition # import prebuild components like ToolNode And conditions for tool for tools in graph\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults # import TavilySearchResults class for handiling search results from Travi\n",
    "from langchain_community.tools.riza.command import  ExecPython # import ExecPython class for Executing python code with riza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001E8FA609DC0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001E8FA60C2F0>, model_name='llama-3.2-90b-vision-preview', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(groq_api_key = Groq_API_Key , model_name=  \"llama-3.2-90b-vision-preview\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_tavily =  TavilySearchResults(max_results=2)\n",
    "tools_code_interpreter = ExecPython()\n",
    "tools = [tools_tavily,tools_code_interpreter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools = tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response Content: The output of `print(['a']+[2])` will be:\n",
      "\n",
      "```python\n",
      "['a', 2]\n",
      "```\n",
      "\n",
      "This is because in Python, you can concatenate lists using the `+` operator. The resulting list will contain all elements from the first list (`['a']`) followed by all elements from the second list (`[2]`).\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke('''### what is the output of print(['a']+[2])''')\n",
    "if hasattr(response, 'content'):\n",
    "    print(\"AI Response Content:\", response.content)\n",
    "else:\n",
    "    print(\"AI Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a typed Dictionary to represent the State Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages:Annotated[list,add_messages] # List of messages, With add_message function applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a StateGraph instance,Specying State type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a chatbot function that takes a state as input and returns a new state with updated messages\n",
    "def chatBot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}   # Invoke the LLM on the input messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_Builder = StateGraph(State)\n",
    "graph_Builder.add_node(\"chatBot\",chatBot)\n",
    "tool_node = ToolNode(tools=tools)\n",
    "graph_Builder.add_node(\"tools\",tool_node)\n",
    "graph_Builder.add_conditional_edges(\n",
    "    \"chatBot\",\n",
    "    tools_condition,\n",
    ")\n",
    "# Any time a tool is called we return chatBot to decide next step\n",
    "\n",
    "graph_Builder.add_edge(\"tools\",\"chatBot\")\n",
    "graph_Builder.set_entry_point(\"chatBot\")\n",
    "graph = graph_Builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Graph based on user input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    # Create an initial state with the user's input message\n",
    "    initial_state = {\"messages\": [('user', user_input)]}  # Fixed key definition\n",
    "\n",
    "    # Stream updates from the compiled graph\n",
    "    for event in graph.stream(initial_state):\n",
    "        # Loop through each event's values (should be only one in this case)\n",
    "        for value in event.values():\n",
    "            # Print Assistant's latest message\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I'm happy to chat with you. Is there something I can help you with or would you like to ask a question?\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"user :\") # Get input from the user\n",
    "\n",
    "        if user_input.lower() in [\"quit\",\"q\",\"exit\"]:\n",
    "            print(\"Good Bye!\")\n",
    "            break # Exit the loop\n",
    "\n",
    "        stream_graph_updates(user_input) # process the input using this graph\n",
    "    except Exception as e:\n",
    "        print(\"Exception is Occured during processing :\",e)\n",
    "        stream_graph_updates(user_input)\n",
    "        break # exit the loop after the fallback\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
